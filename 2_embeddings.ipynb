{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Getting Embeddings from XLM RoBERTa\n",
    "Created by: Sebastián Vallejo V.\n",
    "\n",
    "Updated: July 8 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import transformers\n",
    "from torch.utils.data import TensorDataset\n",
    "import time\n",
    "import datetime\n",
    "import subprocess\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time:\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
    "\n",
    "# check gpu usage:\n",
    "def show_gpu(msg):\n",
    "    \"\"\"\n",
    "    ref: https://discuss.pytorch.org/t/access-gpu-memory-usage-in-pytorch/3192/4\n",
    "    \"\"\"\n",
    "    def query(field):\n",
    "        return(subprocess.check_output(\n",
    "            ['nvidia-smi', f'--query-gpu={field}',\n",
    "                '--format=csv,nounits,noheader'],\n",
    "            encoding='utf-8'))\n",
    "    def to_int(result):\n",
    "        return int(result.strip().split('\\n')[0])\n",
    "\n",
    "    used = to_int(query('memory.used'))\n",
    "    total = to_int(query('memory.total'))\n",
    "    pct = used/total\n",
    "    print('\\n' + msg, f'{100*pct:2.1f}% ({used} out of {total})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load models. Choose models depending on GPU capabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XLMRobertaModel.from_pretrained(\"xlm-roberta-large\")\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-large\", do_lower_case=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import dataframe.\n",
    "The models use up all of the GPU memory so I have to do it in chuncks. I also have to reset the indices every time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(\"/Users/sebastian/OneDrive - University Of Houston/Papers and Chapters/Gendered Speech in Gendered Institutions/Data/data_styles\")\n",
    "data = pd.read_excel(r\"speeches_sent_14.xlsx\")\n",
    "data = data.iloc[0:10000]\n",
    "data = data.drop(3004)\n",
    "data = data.reset_index()\n",
    "data = data.drop('index', 1)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create tokenized data for tensor dataset. Adjust max_len depending on GPU capabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attn_masks = []\n",
    "\n",
    "max_len = 250\n",
    "\n",
    "for x, row in data.iterrows():\n",
    "    encoded_dict = tokenizer.encode_plus(row['text'],\n",
    "    add_special_tokens = False,\n",
    "    max_length=max_len, \n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_tensors='pt',\n",
    "    is_split_into_words=True)\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    attn_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Convert into tensor matrix (required step for TensorDataset, which helps with batching).\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attn_masks = torch.cat(attn_masks, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch seeds for replicability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_val = 1984\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "torch.cuda.empty_cache() #Clear GPU cache if necessary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the model parameters. Adjust batch_size depending on GPU capabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Settings:\n",
    "    seed = 1984\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare tensor dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(input_ids, attn_masks)\n",
    "train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=Settings.batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we start the training to obtain the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_batch = torch.Tensor([])\n",
    "iter_dataloader = iter(train_dataloader)\n",
    "total_t0 = time.time() # Measure the total training time\n",
    "\n",
    "show_gpu('Initial GPU memory usage:') ### Just testing memory usage>\n",
    "model.train()\n",
    "show_gpu('GPU memory usage after loading training objects:') ### Just testing memory usage>\n",
    "\n",
    "print(\"Start of model\")\n",
    "\n",
    "for batch in enumerate(train_dataloader):\n",
    "\n",
    "    t0 = time.time() # Start timer\n",
    "    batch = iter_dataloader.next()\n",
    "\n",
    "    ### Create batches\n",
    "    b_input_ids = batch[0].to(Settings.device)\n",
    "    b_input_mask = batch[1].to(Settings.device)\n",
    "\n",
    "    # Always clear any previously calculated gradients before performing a backward pass.\n",
    "    model.zero_grad()\n",
    "\n",
    "    ### Send model to device\n",
    "    model.to(Settings.device)\n",
    "\n",
    "    ### Run model\n",
    "    output = model(b_input_ids, b_input_mask)\n",
    "\n",
    "    # Clip the norm of the gradients to 1.0. This is to help prevent the \"exploding gradients\" problem.\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "    ### save last_hidden_state\n",
    "    embeddings_batch = torch.cat((embeddings_batch.detach().cpu(),output[0].detach().cpu())) # Detach otherwise OUT OF MEMORY\n",
    "\n",
    "    ### Just testing memory usage>\n",
    "    show_gpu('GPU memory usage after training batch:')\n",
    "    torch.cuda.empty_cache() #Clear GPU cache if necessary # Not sure if this is neessary at this stage\n",
    "    show_gpu('GPU memory usage after clearing cache:')\n",
    "\n",
    "#######\n",
    "print(\"End of model\")\n",
    "# Delete model to make room un memory:\n",
    "show_gpu('GPU memory usage after training:') ### Just testing memory usage>\n",
    "del model\n",
    "torch.cuda.empty_cache() #Clear GPU cache if necessary # Not sure if this is neessary at this stage\n",
    "show_gpu('GPU memory usage after deleting model:') ### Just testing memory usage>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each observation in embeddings_batch is a speech. To obtain the embeddings for each word we run a loop through each individual observation to create a matrix where each column is a word (token), each row is a speech (speech_id) and each cell is an embedding. In BERT every word is broken up into smaller bytes, so we have to put them back together again and average the embeddings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"shape:\", embeddings_batch.shape)\n",
    "\n",
    "mask_pd = pd.DataFrame(attn_masks.numpy()).transpose()\n",
    "mask_pd.head(20) # We only need the information from the tokens and not the padding\n",
    "\n",
    "embeddings_final = pd.DataFrame(columns =['text_id'])\n",
    "print(\"Start of loop\")\n",
    "\n",
    "for i in range(len(embeddings_batch)): # Start loop\n",
    "    print(\"For loop number\", i, sep=\" \")\n",
    "\n",
    "    # Get embeddings\n",
    "    embeddings = embeddings_batch[i,:,:].detach() # We keep only the embeddings and remove gradient information\n",
    "    embeddings_pd = pd.DataFrame(embeddings.cpu().numpy())\n",
    "\n",
    "    embeddings_short = embeddings_pd.loc[:len(mask_pd.loc[mask_pd[i] == 1])]\n",
    "\n",
    "    row_names_temp = pd.DataFrame(tokenizer.convert_ids_to_tokens(input_ids[i]))\n",
    "    row_names_temp = row_names_temp.loc[:len(mask_pd.loc[mask_pd[i] == 1])]\n",
    "    row_names_temp.loc[row_names_temp[0] == \"<pad>\", 0 ] = '▁pad▁'\n",
    "\n",
    "    # Add tokens to each vectors\n",
    "    embeddings_short.index = row_names_temp[0]\n",
    "\n",
    "    ## combine syllables that are separated by estimating the mean\n",
    "    embeddings_mean = pd.DataFrame()\n",
    "    temp = pd.DataFrame()\n",
    "    index_temp = []\n",
    "    index_names = list('')\n",
    "\n",
    "    len_index = len(embeddings_short.index)-1\n",
    "    print(\"Index Length:\", len_index, sep=\" \")\n",
    "\n",
    "    for index in range(len_index): # One less than the length in order to not go out of bounds\n",
    "        temp = pd.DataFrame()\n",
    "        index_temp = pd.DataFrame()\n",
    "\n",
    "        temp = temp.append(embeddings_short.iloc[index], ignore_index=True)\n",
    "        index_temp = embeddings_short.index[index]\n",
    "\n",
    "        if \"▁\" not in embeddings_short.index[index+1]: # see if the next one does not have a _ that means it is part of the same word\n",
    "            temp = temp.append(embeddings_short.iloc[index+1], ignore_index=True)\n",
    "            index_temp = index_temp + embeddings_short.index[index + 1] # combine the syllables\n",
    "            if \"▁\" not in embeddings_short.index[index+2]:\n",
    "                temp = temp.append(embeddings_short.iloc[index+2], ignore_index=True)\n",
    "                index_temp = index_temp + embeddings_short.index[index + 2]\n",
    "                if \"▁\" not in embeddings_short.index[index+3]:\n",
    "                    temp = temp.append(embeddings_short.iloc[index+3], ignore_index=True)\n",
    "                    index_temp = index_temp + embeddings_short.index[index + 3]\n",
    "                    if \"▁\" not in embeddings_short.index[index+4]:\n",
    "                        temp =  temp.append(embeddings_short.iloc[index+4], ignore_index=True)\n",
    "                        index_temp = index_temp + embeddings_short.index[index + 4]\n",
    "                        embeddings_mean= embeddings_mean.append(temp.mean(), ignore_index=True)\n",
    "                        index_names.append(index_temp)\n",
    "                    else:\n",
    "                        embeddings_mean= embeddings_mean.append(temp.mean(), ignore_index=True)\n",
    "                        index_names.append(index_temp)\n",
    "                else:\n",
    "                    embeddings_mean= embeddings_mean.append(temp.mean(), ignore_index=True)\n",
    "                    index_names.append(index_temp)\n",
    "            else:\n",
    "                embeddings_mean= embeddings_mean.append(temp.mean(), ignore_index=True)\n",
    "                index_names.append(index_temp)\n",
    "        else:\n",
    "            embeddings_mean= embeddings_mean.append(temp.mean(), ignore_index=True)\n",
    "            index_names.append(index_temp)\n",
    "\n",
    "    embeddings_mean.index = index_names\n",
    "    embeddings_mean = embeddings_mean.append(embeddings_short.iloc[len(embeddings_short)-1]) # add the last row (_end_) which will always be missing from the loop\n",
    "\n",
    "    ## Erase all the words that do not start with _\n",
    "    embeddings_mean = embeddings_mean[embeddings_mean.index.str.contains(\"▁\") == True]\n",
    "\n",
    "    ## Mean of repeated words **** Might need to think more about this (e.g. what if there is word play) **** \n",
    "    ## But it seems fine if the UoA is a sentences.\n",
    "    embeddings_mean.index.name = 'tokens'\n",
    "    embeddings_mean = embeddings_mean.groupby('tokens', as_index=True).mean()\n",
    "\n",
    "    ## aggregate to make a list of embeddings\n",
    "    embeddings_agg = embeddings_mean.aggregate(lambda x: [x.tolist()], axis=1)\n",
    "    embeddings_agg = pd.DataFrame(embeddings_agg)\n",
    "    embeddings_agg = embeddings_agg.reset_index()\n",
    "    embeddings_agg.columns = ['text_id', data['text_id'][i]] # Add names to columns for merge\n",
    "\n",
    "    ## Bind all\n",
    "    embeddings_final = pd.merge(embeddings_final, embeddings_agg, how=\"outer\",on='text_id')\n",
    "\n",
    "print(\"End of loop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep the usual shape of a dfm object, we transpose the matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_final_t = embeddings_final.transpose()\n",
    "embeddings_final_t.columns = embeddings_final_t.iloc[0]\n",
    "embeddings_final_t = embeddings_final_t.iloc[1: , :]\n",
    "embeddings_final_t = embeddings_final_t.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and continue in R:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_final_t.to_excel('short_emb.xlsx')\n",
    "# embeddings_final_t.to_feather('test.feather')\n",
    "embeddings_final_t.to_parquet('speeches_emb_14_1.parquet', compression='BROTLI') # Smallest file size"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
